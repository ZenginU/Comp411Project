{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOU_BLV1D6OE"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "content_layer = \"conv4_2\"\n",
    "\n",
    "style_weights = {'conv1_1': 1.,\n",
    "                 'conv2_1': 0.75,\n",
    "                 'conv3_1': 0.5,\n",
    "                 'conv4_1': 0.3,\n",
    "                 'conv5_1': 0.1}\n",
    "\n",
    "content_weight = 1  # alpha\n",
    "style_weight = 1e6  # beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    b, d, h, w = tensor.size()\n",
    "    tensor = tensor.view(d, h*w)\n",
    "    return torch.mm(tensor, tensor.t()) \n",
    "\n",
    "\n",
    "class StyleTransferModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleTransferModel, self).__init__()\n",
    "        self.model = models.vgg19(weights=\"DEFAULT\").features\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.model.to(device)\n",
    "\n",
    "    def forward(self, image):\n",
    "        layers = {'0': 'conv1_1',\n",
    "                 '5':  'conv2_1',\n",
    "                 '10': 'conv3_1',\n",
    "                 '19': 'conv4_1',\n",
    "                 '21': 'conv4_2',\n",
    "                 '28': 'conv5_1'}\n",
    "            \n",
    "        features = {}\n",
    "        x = image\n",
    "    \n",
    "        for i, layer in enumerate(self.model):\n",
    "            name = str(i)\n",
    "            x = layer(x)\n",
    "            if name in layers:\n",
    "                features[layers[name]] = x\n",
    "                \n",
    "        return features\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, style_features, style_weights):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.style_grams = {layer: gram_matrix(style_features[layer]).detach() for layer in style_features}\n",
    "        self.style_weights = style_weights\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, target_features):\n",
    "        style_loss = 0\n",
    "        for layer in self.style_weights:\n",
    "            # Get the \"target\" style representation for the layer\n",
    "            target_feature = target_features[layer]\n",
    "            _, d, h, w = target_feature.shape\n",
    "            target_gram = gram_matrix(target_feature)\n",
    "    \n",
    "            layer_style_loss = style_weights[layer] * nn.functional.mse_loss(target_gram, self.style_grams[layer])\n",
    "            \n",
    "            style_loss += layer_style_loss / (d * h * w)\n",
    "        return style_loss\n",
    "        \n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target_feature.detach()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.mse_loss(x, self.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwGTjjCMD6Oe"
   },
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=400, shape=None):\n",
    "    \n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    size = min(max(image.size), max_size)\n",
    "    \n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "        \n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.Resize(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    return in_transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lV4AH12RD6On"
   },
   "outputs": [],
   "source": [
    "# load in content and style image\n",
    "content = load_image('content.jpg').to(device)\n",
    "\n",
    "style = load_image('style.jpg', shape=content.shape[-2:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vR6wIgRSD6Ot"
   },
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "\n",
    "    image = tensor.clone().detach().cpu().numpy()\n",
    "    image = image.squeeze().transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gdm4NLTeD6PQ"
   },
   "source": [
    "## Content and Style Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egf0LVGUD6Ps",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "style_transfer_model = StyleTransferModel().to(device)\n",
    "\n",
    "content_features = style_transfer_model(content)\n",
    "style_features = style_transfer_model(style)\n",
    "\n",
    "target = content.clone().requires_grad_(True).to(device)\n",
    "#target = torch.randn_like(content).clone().requires_grad_(True).to(device)\n",
    "\n",
    "content_loss_module = ContentLoss(content_features[content_layer]).to(device)\n",
    "style_loss_module = StyleLoss(style_features, style_weights).to(device)\n",
    "\n",
    "optimizer = optim.LBFGS([target])\n",
    "\n",
    "show_every = 10\n",
    "STEPS = 30\n",
    "\n",
    "losses = list()\n",
    "\n",
    "def closure():\n",
    "    # Calculate the content loss\n",
    "    target_features = style_transfer_model(target)\n",
    "    \n",
    "    content_loss = content_loss_module(target_features[content_layer])\n",
    "    \n",
    "    style_loss = style_loss_module(target_features)\n",
    "        \n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "\n",
    "    losses.append(total_loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# Optimization loop using L-BFGS\n",
    "for ii in range(1, STEPS+1):\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    # Display intermediate images and print the loss\n",
    "    if ii % show_every == 0:\n",
    "        print('Iteration {}, Total loss: {}'.format(ii, closure().item()))\n",
    "        plt.imshow(im_convert(target))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni7Q99l1D6Px"
   },
   "source": [
    "## Display the Target Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9poOzV0yD6Py"
   },
   "outputs": [],
   "source": [
    "# display content and final, target image\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(style))\n",
    "ax3.imshow(im_convert(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(losses)), losses, label='Total Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
